# Databricks notebook source
# DBTITLE 1,olo
# MAGIC %md
# MAGIC # Incremental Data LOading Using Auto Loader

# COMMAND ----------

# MAGIC %sql
# MAGIC CREATE SCHEMA netflix_catalog_devv.net_schema;

# COMMAND ----------

checkpoint_location = "abfss://silver@netflixprojectdldevraj.dfs.core.windows.net/checkpoints"

# COMMAND ----------

df = spark.readStream\
  .format("cloudFiles")\
  .option("cloudFiles.format", "csv")\
  .option("cloudFiles.schemaLocation", checkpoint_location)\
  .load("abfss://raw@netflixprojectdldevraj.dfs.core.windows.net")

# COMMAND ----------

display(df)

# COMMAND ----------

 df.writeStream\
  .option("checkpointLocation", checkpoint_location)\
  .trigger(processingTime='10 seconds')\
  .start("abfss://bronze@netflixprojectdldevraj.dfs.core.windows.net/netflix_titles")

# COMMAND ----------


# Databricks notebook source
# MAGIC %md
# MAGIC # Silver Notebook Lookup Tables

# COMMAND ----------

# MAGIC %md
# MAGIC ### Parameters

# COMMAND ----------

dbutils.widgets.text("sourcefolder","netflix_directors")
dbutils.widgets.text("targetfolder","netflix_directors")

# COMMAND ----------

# MAGIC %md
# MAGIC ### Variables

# COMMAND ----------

var_src_folder = dbutils.widgets.get("sourcefolder")
var_tgt_folder = dbutils.widgets.get("targetfolder")

# COMMAND ----------

df = spark.read.format("csv")\
        .option("header", True)\
        .option("inferSchema", True)\
        .load(f"abfss://bronze@netflixprojectdldevraj.dfs.core.windows.net/{var_src_folder}")

# COMMAND ----------

df.display()

# COMMAND ----------

df.write.format("delta")\
    .mode("append")\
    .option("path", f"abfss://silver@netflixprojectdldevraj.dfs.core.windows.net/{var_tgt_folder}")\
    .save()
# Databricks notebook source
# MAGIC %md
# MAGIC Array Parameter

# COMMAND ----------

files = [
    {
        "sourcefolder" : "netflix_directors",
        "targetfolder" : "netflix_directors"
    },
    {
        "sourcefolder" : "netflix_cast",
        "targetfolder" : "netflix_cast"
    },
    {
        "sourcefolder" : "netflix_countries",
        "targetfolder" : "netflix_countries"
    },
    {
        "sourcefolder" : "netflix_category",
        "targetfolder" : "netflix_category"
    },
]

# COMMAND ----------

# MAGIC %md
# MAGIC ### Job Utility to return the array

# COMMAND ----------

dbutils.jobs.taskValues.set(key = "my_arr", value = files)
# Databricks notebook source
from pyspark.sql.functions import *
from pyspark.sql.types import *

# COMMAND ----------

# MAGIC %md
# MAGIC # Silver Data Transformation

# COMMAND ----------

df = spark.read.format("delta")\
    .option("header", True)\
    .option("inferSchema", True)\
    .load("abfss://bronze@netflixprojectdldevraj.dfs.core.windows.net/netflix_titles")

# COMMAND ----------

df.display()

# COMMAND ----------

df = df.fillna({"duration_minutes": 0, "duration_seasons": 1})

# COMMAND ----------

df = df.withColumn("duration_minutes", col("duration_minutes").cast(IntegerType()))\
            .withColumn("duration_seasons", col("duration_seasons").cast(IntegerType()))

# COMMAND ----------

df.printSchema()

# COMMAND ----------

df = df.withColumn("Short_title",split(col("title"), ":").getItem(0))
df.display()

# COMMAND ----------

df = df.withColumn("rating",split(col("rating"), "-").getItem(0))
df.display()

# COMMAND ----------

# reload the silver dataset (full data)
df_full = spark.read.format("delta").load("abfss://bronze@netflixprojectdldevraj.dfs.core.windows.net/netflix_titles/")

print("Full row count:", df_full.count())

# now group by type
from pyspark.sql.functions import col, count, trim

df_grouped = (df_full
              .withColumn("type", trim(col("type")))
              .groupBy("type")
              .agg(count("*").alias("total_count")))

df_grouped.show()

# COMMAND ----------

from pyspark.sql.functions import col, trim

# Clean whitespaces and filter only valid values
df = df.withColumn("type", trim(col("type"))) \
       .filter(col("type").isin("Movie", "TV Show"))

df = df.withColumn(
    "type_flag",
    when(col("type") == "Movie", 1)
    .when(col("type") == "TV Show", 2)
)

# Now group
df_grouped = df.groupBy("type", "type_flag").agg(count("*").alias("total_count"))

df_grouped.display()

# COMMAND ----------

from pyspark.sql.window import Window
df = df.withColumn("duration_ranking",dense_rank().over(Window.orderBy(col("duration_minutes").desc())))

# COMMAND ----------

df.display()

# COMMAND ----------

df.createOrReplaceTempView("temp_view")

# COMMAND ----------

df.createOrReplaceGlobalTempView("global_view")

# COMMAND ----------

df = spark.sql("""
               select * from global_temp.global_view
               
               """)

# COMMAND ----------

df = spark.sql("""select * from temp_view""")
df.display()

# COMMAND ----------

from pyspark.sql.functions import count, col, trim

# reload full data
df_full = spark.read.format("delta").load("abfss://bronze@netflixprojectdldevraj.dfs.core.windows.net/netflix_titles/")

print("Full row count:", df_full.count())  # should be thousands, not 2

# clean 'type' column (remove spaces)
df_full = df_full.withColumn("type", trim(col("type")))

# group by type
df_grouped = df_full.groupBy("type").agg(count("*").alias("total_count"))
df_grouped.display()

# COMMAND ----------

df = df.groupBy("type").agg(count("*").alias("total_count"))
df.display()

# COMMAND ----------

df.write.format("delta").mode("overwrite").option("path","abfss://silver@netflixprojectdldevraj.dfs.core.windows.net/netflix_titles").save()

# COMMAND ----------


# Databricks notebook source
dbutils.widgets.text("weekday","7")

# COMMAND ----------

var = int(dbutils.widgets.get("weekday"))

# COMMAND ----------

dbutils.jobs.taskValues.set(key="weekoutput", value=var)

# COMMAND ----------

var = int(dbutils.widgets.get("weekday"))

# Save as task value for downstream tasks
dbutils.jobs.taskValues.set(key="weekoutput", value=str(var))


# COMMAND ----------

weekday_val = dbutils.jobs.taskValues.get(
    taskKey="Weekday_lookup", 
    key="weekoutput", 
    debugValue="7"   # fallback if upstream didn't run
)
print(weekday_val)
# Databricks notebook source
# MAGIC %md
# MAGIC # DLT Notebook - GOLD LAYER

# COMMAND ----------

looktables_rules = {
    "rule1" : "show_id is NOT NULL"
    }

# COMMAND ----------

@dlt.table(
    name = "gold_netflixdirectors"
)

@dlt.expect_all_or_drop(looktables_rules)
def myfunc():
    df = spark.readstream.format("delta").load("abfss://silver@netflixprojectdldevraj.dfs.core.windows.net/netflix_directors")
    return df

# COMMAND ----------

@dlt.table(
    name = "gold_netflixcast"

)

@dlt.expect_all_or_drop(looktables_rules)
def myfunc():
    df = spark.readstream.format("delta").load("abfss://silver@netflixprojectdldevraj.dfs.core.windows.net/netflix_cast")
    return df

# COMMAND ----------

@dlt.table(
    name = "gold_netflixcountries"
)

@dlt.expect_all_or_drop(looktables_rules)
def myfunc():
    df = spark.readstream.format("delta").load("abfss://silver@netflixprojectdldevraj.dfs.core.windows.net/netflix_countries")
    return df

# COMMAND ----------

@dlt.table(
    name = "gold_netflixcategory"
)

@dlt.expect_all_or_drop("rule1","show_id is NOT NULL")
def myfunc():
    df = spark.readstream.format("delta").load("abfss://silver@netflixprojectdldevraj.dfs.core.windows.net/netflix_category")
    return df

# COMMAND ----------

@dlt.table

def gold_stg_netflixtitles():

    df = spark.readstream.format("delta").load("abfss://silver@netflixprojectdldevraj.dfs.core.windows.net/netflix_titles")
    return df

# COMMAND ----------

from pyspark.sql.functions import *

# COMMAND ----------

@dlt.view

def gold_trns_netflixtitles():
    df = spark.readstream.table("LIVE.gold_stg_netflixtitles")
    df = df.withcolumn("newflag",lit(1))
    return df

# COMMAND ----------

masterdata_rules = {
    "rule1" : "newflag is NOT NULL",
    "rule2" : "show_id is NOT NULL"
    }

# COMMAND ----------

@dlt.table

@dlt.expect_all_or_drop(masterdata_rules)
def gold_netflixtitles():
    df = spark.readstream.table("LIVE.gold_trns_netflixtitles")
    return df
# Databricks notebook source
var = dbutils.jobs.taskValues.get(
    taskKey="Weekday_lookup",
    key="weekoutput",
    debugValue="default_value"
)

# COMMAND ----------

print(var)
